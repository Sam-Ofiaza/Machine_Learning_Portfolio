---
title: "Logistic Regression on the Kaggle Dataset Medals Data Set"
author: "Sam Ofiaza"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

The Kaggle Dataset Medals data set contains information on datasets including the medal it has received (if any) and how many votes, views, downloads, etc. it has.

Credit to Niek van der Zwaag for the data set ([link](https://www.kaggle.com/datasets/niekvanderzwaag/kaggle-dataset-medals?select=dataset_medal_total.csv)).

### Linear Models for Classification Overview

As opposed to linear models for regression, linear models for classification are those whose target is qualitative. The overall goal for these models is not to find a line of best fit, but to find a line that divides classes. These models have probabilistic results that are easy to interpret and are easy to calculate, update, and scale. However, they are inflexible and simple compared to other classification algorithms. They won't perform well if the data deviates too much from their simplistic assumptions. The two linear models for classification used in this notebook are Logistic Regression and Naive Bayes.

```{r}
df <- read.csv("./dataset_medal_total.csv")
df <- df[, c(2, 5, 6, 7, 8)]
df$Medal <- factor(df$Medal, levels=c("None", "Bronze", "Silver", "Gold"))
str(df)
sapply(df, function(x) sum(is.na(x)==TRUE))
```

```{r}
str(df)
```

```{r}
plot(df$Views, df$Votes, pch=21, bg=c("black", "brown", "lightgray", "gold")[unclass(df$Medal)])
```

```{r}
pairs(df[2:5], pch=21, bg=c("black", "brown", "lightgray", "gold")[unclass(df$Medal)])
```

```{r}
table(df$Medal)/nrow(df)
```

```{r}
gold <- df
gold$Medal <- as.factor(ifelse (df$Medal=="Gold", 1, 0))

silver <- df
silver$Medal <- as.factor(ifelse (df$Medal=="Silver", 1, 0))

bronze <- df
bronze$Medal <- as.factor(ifelse (df$Medal=="Bronze", 1, 0))

none <- df
none$Medal <- as.factor(ifelse (df$Medal=="None", 1, 0))
```

```{r}
fun1 <- function(df, i) {
  train <- df[i,]
  glm1 <- glm(Medal~., data=train, family="binomial")
  glm1
}

fun2 <- function(glm, df, i) {
  test <- df[-i,]
  probs <- predict(glm1, newdata=test)
  pred <- ifelse(probs>0.5, 1, 0)
  acc <- mean(pred==test$Medal)
  print(paste("accuracy = ", acc))
  table(pred, test$Medal)
}
```


```{r}
set.seed(4321)
i <- sample(1:nrow(df), nrow(df)*0.8, replace=FALSE)
```

```{r}
glm1 <- fun1(gold, i)
summary(glm1)
```

```{r}
glm2 <- fun1(silver, i)
summary(glm2)
```

```{r}
glm3 <- fun1(bronze, i)
summary(glm3)
```

```{r}
glm4 <- fun1(none, i)
summary(glm4)
```

### Logistic Regression Model Summary Analysis

The estimate column in the coefficients table indicates the change in log odds for each predictor in receiving the model's corresponding positive class (its specific medal). The four summaries correspond to the four classes gold, silver, bronze, and none respectively. For example, in the above summary for receiving no medals, receiving one more view will decrease the log odds of receiving no medal by -1.578e9. Each additional vote will increase the log odds of receiving no medal by 9.613e12. The p-value for each predictor in each model is significant in determining its model's medal except every predictor in gold's model and Downloads in silver and bronze's models. A lower residual deviance value compared to the null deviance value indicates that adding the predictors helped the model compared to just having the intercept. It seems that this occurred in every model, particularly in gold's case where the difference is several orders of magnitude higher compared to the rest. All of this indicates that the models are all good.

```{r}
library(e1071)
train <- df[i,]
test <- df[-i,]
nb1 <- naiveBayes(train[,2:5], train[,1], laplace=laplace)
nb1
```

### Naive Bayes Model Summary Analysis

The A-priori probabilities are the estimated probabilities of each class before using any predictor. Unsurprisingly, receiving no medal is most likely for most datasets. Each conditional probability table for each predictor indicates how the binomial distribution of that predictor appears. The columns [,1] and [,2] are the mean and variance, respectively. 

```{r}
fun2(glm1, gold, i)
```

```{r}
fun2(glm2, silver, i)
```

```{r}
fun2(glm3, bronze, i)
```

```{r}
fun2(glm4, none, i)
```

```{r}
p1 <- predict(nb1, newdata=test)
A <- table(p1, test$Medal)
A[order(A[,1], decreasing=TRUE), order(A[,1], decreasing=TRUE)]
```

### Analysis of Prediction Results on Test Data

The one vs many approach used in Logistic Regression worked perfectly for gold, very well for silver, decent for bronze, and horribly for none based on accuracy and the confusion matrices. The confusion matrix for none in particular contains a high amount of false positives. The Naive Bayes results seem better overall by losing accuracy for gold and silver and gaining accuracy for none and bronze compared to the results for logistic regression. I believe that Logistic Regression performed poorly because it was unable to distinguish clear boundaries between the none class, which is ~80% of the data, and the other classes.

### Strengths and Weaknesses of Logistic Regression and Naive Bayes

Logistic Regression has easy to interpret probabilistic results and can be optimized to reduce error with gradient descent. It is better than Naive Bayes when used with large data sets. It does not do well when the classes are muddied together and cannot be cleanly separated with linear boundaries. Naive Bayes works very well for being so simple and can be easy to implement despite the fact that its assumption of independent predictors is often untrue. However, its results can be beaten with more complex models. It has higher bias and lower variance compared to Logistic Regression.

### Benefits and Drawbacks of the Classification Metrics

Accuracy is how often the model correctly predicts the data and is simple and commonly used. The confusion matrix goes into more detail with how often true positives, false positives, true negatives, and false negatives were calculated and can give a better picture as to the performance of the model.